{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_text = \"Zoning Administrator to be notified of Violations: Whenever a violation of this ordinance is known or suspected to exist or expected to be committed, any person may so notify the zoning administrator. All officers and agencies of the City of Rochester shall notify the zoning administrator of any information which suggests that a violation exists or is expected to be committed.\"\n",
    "original_text = \"Subdivision 1. It is the purpose of this chapter to provide for the regulation of uses, buildings, structures or lots which lawfully existed prior to the effective date of this ordinance but which fail to comply with one or more of the applicable regulations or standards established by this ordinance or subsequent amendment of this ordinance, or which have been rendered nonconforming due to circumstances which were not self-created. It is the intent of these regulations to specify those circumstances and conditions under which such nonconformities shall be permitted to continue. Buildings or structures which are now in existence and which were constructed in compliance with the terms of the regulations of some other public entity but became nonconforming upon the annexation to the City, and which are not in compliance with the terms of this code are hereby designated as legal nonconforming buildings or structures. Subd. 2. A municipality may, by ordinance, permit an expansion or impose upon nonconformities reasonable regulations to prevent  and abate nuisances and to protect the public health, welfare, or safety. Subd. 3. The basic policy of this chapter is to allow the continuation of any nonconformity and the normal repair, replacement, restoration, maintenance, or improvement thereof, and to encourage their move toward conformity when the opportunity arises through discontinuance or destruction. In certain cases nonconformities may be permitted to be upgraded when it can be shown that such action will not be harmful and will be beneficial to the surrounding properties, the neighborhood, or the community; and that the goals of local plans will not be impeded by the continuation of the nonconformity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "###########    Extraction w/ NLTK   #############\n",
    "#################################################\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readline()\n",
    "    file.close()\n",
    "    # print(filedata)\n",
    "    article = filedata.split(\". \")\n",
    "    # print(article)\n",
    "    sentences = []\n",
    "\n",
    "    # print(\"Sentence\")\n",
    "    # for sentence in filedata:\n",
    "    #     print(sentence)\n",
    "    #     sentences.append(sentence.split(\" \"))\n",
    "    #     # sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    # sentences.pop() \n",
    "    \n",
    "    return article\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# Basically just finds sentences that are the most similar & spits them back out as the \"summary\"\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Read text and split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    # print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n if top_n <= len(ranked_sentence) else len(ranked_sentence)):\n",
    "    #   summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "      summarize_text.append(ranked_sentence[i][1])\n",
    "\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "generate_summary( \"non-conformatitie_legislative_intent.txt\", 2)\n",
    "# generate_summary( \"zoning-enforcement.txt\", 5)\n",
    "# generate_summary( \"zoning-use-type.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "###########    Extraction w/ LSA   ##############\n",
    "#################################################\n",
    "\n",
    "# Import the summarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# Parsing the text string using PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "parser=PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "# creating the summarizer\n",
    "lsa_summarizer=LsaSummarizer()\n",
    "lsa_summary= lsa_summarizer(parser.document,3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lsa_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "##########   Abstraction w/ BART    #############\n",
    "#################################################\n",
    "\n",
    "# Importing the model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "\n",
    "# Loading the model and tokenizer for bart-large-cnn\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Encoding the inputs and passing them to model.generate()\n",
    "inputs = tokenizer.batch_encode_plus([original_text],return_tensors='pt')\n",
    "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "# Decoding and printing the summary\n",
    "bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(bart_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8aac0de37a317ddf2f48ab18087589e2a475528472d4786ef89be0b510bbdd65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
